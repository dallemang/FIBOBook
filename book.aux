\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{Preface}}{vii}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{Acknowledgments}}{ix}{chapter*.3}}
\citation{tesniere59}
\citation{weber-97}
\HyPL@Entry{10<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}{Information Theory and Rate Distortion Theory}}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch01}{{1}{1}{Information Theory and Rate Distortion Theory}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}{Introduction}}{1}{section.1.1}}
\newlabel{ch01.sec1}{{1.1}{1}{Introduction}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Communication system block diagram.\relax }}{1}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ch01.fig1}{{1.1}{1}{Communication system block diagram.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}{Entropy and Average Mutual Information}}{1}{section.1.2}}
\newlabel{ch01.sec2}{{1.2}{1}{Entropy and Average Mutual Information}{section.1.2}{}}
\newlabel{ch01.eq1}{{1.1}{1}{Entropy and Average Mutual Information}{equation.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Communication system block diagram.\relax }}{2}{figure.caption.5}}
\newlabel{ch01.fig2}{{1.2}{2}{Communication system block diagram.\relax }{figure.caption.5}{}}
\newlabel{fig:bioneuron}{{1.3a}{2}{Annotated visualization of the structure of a biological neuron, reconstructed from electron microscope images of 30nm-thick slices of a mouse brain~\cite {tesniere59}.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:bioneuron}{{a}{2}{Annotated visualization of the structure of a biological neuron, reconstructed from electron microscope images of 30nm-thick slices of a mouse brain~\cite {tesniere59}.\relax }{figure.caption.6}{}}
\newlabel{fig:ap}{{1.3b}{2}{The shape of an action potential. A small external voltage stimulus (blue) triggers a cascade of charge build-up inside a neuron (red) via voltage-gated ion channels. The activation threshold is shown as a dotted line. Simulated using a Hodgkin-Huxley model of a neuron~\protect \cite {weber-97}.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:ap}{{b}{2}{The shape of an action potential. A small external voltage stimulus (blue) triggers a cascade of charge build-up inside a neuron (red) via voltage-gated ion channels. The activation threshold is shown as a dotted line. Simulated using a Hodgkin-Huxley model of a neuron~\protect \cite {weber-97}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces The structure and operation of biological neurons.\relax }}{2}{figure.caption.6}}
\newlabel{ch01.ex1}{{1.2}{2}{Entropy and Average Mutual Information}{ucount.1.2}{}}
\newlabel{ch01.eq2}{{1.2}{2}{Entropy and Average Mutual Information}{equation.1.2.2}{}}
\newlabel{ch01.pro1}{{1.3}{2}{Entropy and Average Mutual Information}{ucount.1.3}{}}
\newlabel{ch01.pro2}{{1.4}{2}{Entropy and Average Mutual Information}{ucount.1.4}{}}
\citation{Boeringer}
\citation{Wen-ChungLiu2005}
\citation{WenWangetal2005}
\newlabel{ch01.ex2}{{1.5}{3}{Entropy and Average Mutual Information}{ucount.1.5}{}}
\newlabel{ch01.eq3}{{1.3}{3}{Entropy and Average Mutual Information}{equation.1.2.3}{}}
\newlabel{ch01.ex3}{{1.6}{3}{Entropy and Average Mutual Information}{ucount.1.6}{}}
\newlabel{ch01.th1}{{1.7}{3}{Entropy and Average Mutual Information}{ucount.1.7}{}}
\newlabel{ch01.th2}{{1.8}{3}{Entropy and Average Mutual Information}{ucount.1.8}{}}
\newlabel{subfig:linclass}{{1.4a}{3}{Points in $\mathbb {R}^2$, subdivided by a single linear classifier. One simple way of understanding linear classifiers is as a line (or hyper-plane, in higher dimensions) which splits space into two regions. In this example, points above the line are mapped to class 1 (red); those below, to class 0 (blue).\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:linclass}{{a}{3}{Points in $\mathbb {R}^2$, subdivided by a single linear classifier. One simple way of understanding linear classifiers is as a line (or hyper-plane, in higher dimensions) which splits space into two regions. In this example, points above the line are mapped to class 1 (red); those below, to class 0 (blue).\relax }{figure.caption.7}{}}
\newlabel{subfig:circclass}{{1.4b}{3}{Points in $\mathbb {R}^2$, subdivided by a combination of four linear classifiers. Each classifier maps \emph {all} points to class 0 or 1, and an additional linear classifier is used to combine the four. This hierarchical model is strictly more expressive than any linear classifier by itself.\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:circclass}{{b}{3}{Points in $\mathbb {R}^2$, subdivided by a combination of four linear classifiers. Each classifier maps \emph {all} points to class 0 or 1, and an additional linear classifier is used to combine the four. This hierarchical model is strictly more expressive than any linear classifier by itself.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Simple elements can be combined to express more complex relationships. This is one basic tenet of deep neural networks.\relax }}{3}{figure.caption.7}}
\newlabel{fig:classifiers}{{1.4}{3}{Simple elements can be combined to express more complex relationships. This is one basic tenet of deep neural networks.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Timer0 Compare Output Mode, non-PWM Mode\relax }}{4}{table.caption.8}}
\newlabel{ch01.tab1}{{1.1}{4}{Timer0 Compare Output Mode, non-PWM Mode\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}{Jordan Canonical Form}}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch02}{{2}{5}{Jordan Canonical Form}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}{The Diagonalizable Case}}{5}{section.2.1}}
\newlabel{ch02.def1}{{2.1}{5}{The Diagonalizable Case}{ucount.2.1}{}}
\newlabel{ch02.ex1}{{2.2}{5}{The Diagonalizable Case}{ucount.2.2}{}}
\newlabel{ch02.rem1}{{2.3}{5}{The Diagonalizable Case}{ucount.2.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}{An Algorithm}}{6}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch03}{{3}{6}{An Algorithm}{chapter.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Construction of user sessions from Web server logs using $h1$ heuristic\relax }}{6}{algorithm.3.1}}
\newlabel{alg:visitingTimeAlg}{{3.1}{6}{Construction of user sessions from Web server logs using $h1$ heuristic\relax }{algorithm.3.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}{Shaded Areas}}{7}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch04}{{4}{7}{Shaded Areas}{chapter.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}{Math}}{9}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch05}{{5}{9}{Math}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}{Bold Math}}{9}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}{Math Variables}}{9}{section.5.2}}
\citation{carroll98}
\citation{carroll98}
\citation{KD:2004}
\citation{tarvainen82}
\citation{weber-97}
\citation{tesniere59}
\citation{SHP:1986}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}{Citations with Author-Year}}{11}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch06}{{6}{11}{Citations with Author-Year}{chapter.6}{}}
\bibcite{Boeringer}{{1}{2004}{{Boeringer and Werner}}{{D. Boeringer and D. Werner}}}
\bibcite{carroll98}{{2}{1998}{{Carroll et~al.}}{{Carroll, Briscoe, and Sanfilippo}}}
\bibcite{Hoetal2005}{{3}{2005}{{Ho et al}}{{Ho et al}}}
\bibcite{KD:2004}{{4}{2004}{{Kopka and Daly}}{{W. Kopka and P.W. Daly}}}
\bibcite{Levin2002}{{5}{2002}{{Levin }}{{F.S. Levin}}}
\bibcite{Wen-ChungLiu2005}{{6}{2005}{{Liu }}{{W.-C. Liu}}}
\bibcite{SHP:1986}{{7}{1986}{{Sgall et~al.}}{{Sgall, Haji\v {c}ov\'{a}, and Panevov\'{a}}}}
\bibcite{tarvainen82}{{8}{1982}{{Tarvainen }}{{Tarvainen}}}
\bibcite{tesniere59}{{9}{1959}{{Tesni{\`e}re }}{{Tesni{\`e}re}}}
\bibcite{weber-97}{{10}{1997}{{Weber }}{{Weber, H.J.}}}
\bibcite{WenWangetal2005}{{11}{2005}{{Wang et al}}{{W. Wang et al}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{Bibliography}}{14}{chapter*.11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{Author's Biography}}{15}{chapter*.12}}
